---
title: "Homework 6"
author: "Rose Killian"
output: github_document
---

### Loading libraries and data

```{r message= FALSE}
library(tidyverse)
library(modelr)
library(mgcv)


knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(8105)
```

# Problem 1

Load and clean the data for regression analysis (i.e. convert numeric to factor where appropriate, check for missing data, etc.).

```{r, data import tidy}
birth_df = 
  read_csv("data/birthweight.csv") %>% 
    janitor::clean_names() %>% 
  mutate(
    babysex = recode_factor(babysex, `1` = "male", `2` = "female"),
    frace = recode_factor(frace, `1` = "white", `2` = "black", `3` = "asian", `4` = "puerto rican", `8` = "other", `9` = "unknown"),
    mrace = recode_factor(mrace, `1` = "white", `2` = "black", `3` = "asian", `4` = "puerto rican", `8` = "other", `9` = "unknown"),
    malform = recode_factor(malform, `0` = "absent", `1` = "present")
  )

colSums(is.na(birth_df))
```

No missing observations


### Propose a regression model for birthweight.

After doing some general research on known risk-factors of low birth weight I will start to build a model by examining the effects of length at birth, head circumference, gestational age, sex, cigarettes smoked, income, presence of malformations, mother's age, parental race, and mother's pre-pregnancy BMI and weight gain on birth weight.

Based on the literature, I would like to include parity, number of previous low birth weight babies, and number of previous small for gestational age babies, but almost every value for parity is zero (total previous births = `r sum(pull(birth_df, parity))` with range `r min(pull(birth_df, parity))` - `r max(pull(birth_df, parity))`) and no observation in the sample has a previous low birth weight or small for gestational age baby. Therefore, these variables were excluded.

```{r, model 1}
my_mod = lm(bwt ~ blength + bhead + gaweeks + babysex + smoken + fincome + malform + momage + frace + mrace + ppbmi + wtgain, data = birth_df)

my_mod %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value) %>% 
  knitr::kable(digits = 3)
```

Looks like `malform`, `momage` and `frace` are all non-significant. Removing them from the model and re-running:

```{r, model 1.2}
my_mod = lm(bwt ~ blength + bhead + gaweeks + babysex + smoken + fincome + mrace + ppbmi + wtgain, data = birth_df)

my_mod %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value) %>% 
  knitr::kable(digits = 3)

birth_df %>% 
  modelr::add_residuals(my_mod) %>% 
  modelr::add_predictions(my_mod) %>% 
  ggplot(aes(x = pred, y = resid, alpha = 0.3)) +
  geom_point()
```


Compare your model to two others:

* One using length at birth and gestational age as predictors (main effects only)

```{r, model 2}
main_mod = lm(bwt ~ blength + gaweeks, data = birth_df)

main_mod %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value) %>% 
  knitr::kable(digits = 3)

birth_df %>% 
  modelr::add_residuals(main_mod) %>% 
  modelr::add_predictions(main_mod) %>% 
  ggplot(aes(x = pred, y = resid, alpha = 0.3)) +
  geom_point()
  
```


* One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r, model 3}
interaction_mod = lm(bwt ~ blength * bhead * babysex, data = birth_df)

interaction_mod %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value) %>% 
  knitr::kable(digits = 3)

birth_df %>% 
  modelr::add_residuals(interaction_mod) %>% 
  modelr::add_predictions(interaction_mod) %>% 
  ggplot(aes(x = pred, y = resid, alpha = 0.3)) +
  geom_point()
```

### Cross Validation

Make this comparison in terms of the cross-validated prediction error.

```{r, cv}
cv_df = 
  crossv_mc(birth_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_df = 
  cv_df %>% 
  mutate(
   my_mod = map(train, ~lm(bwt ~ blength + bhead + gaweeks + babysex + smoken + fincome + mrace + ppbmi + wtgain, data = .x)),
   main_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
   interaction_mod = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))
  ) %>% 
  mutate(
    rmse_mine = map2_dbl(my_mod, test, ~rmse(model = .x, data = .y)),
    rmse_main = map2_dbl(main_mod, test, ~rmse(model = .x, data = .y)),
    rmse_interaction = map2_dbl(interaction_mod, test, ~rmse(model = .x, data = .y))
  )
```

Plotting the results:

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot()
```

My model has the lowest RMSE of all the models. By this metric my model performs much better than the main effects model and slightly better than the interaction model. The main effects model is the worst fit for the data with the highest RMSE.

# Problem 2

```{r, data import}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

The boostrap is helpful when you’d like to perform inference for a parameter / value / summary that doesn’t have an easy-to-write-down distribution in the usual repeated sampling framework. We’ll focus on a simple linear regression with tmax as the response and tmin as the predictor, and are interested in the distribution of two quantities estimated from these data:

* r^2

* log(β^0∗β^1)

Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities. Plot the distribution of your estimates, and describe these in words. Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r^2
and log(β^0∗β^1). Note: broom::glance() is helpful for extracting r^2 from a fitted regression, and broom::tidy() (with some additional wrangling) should help in computing log(β^0∗β^1).

```{r}
boot_straps = 
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~tmin, data = .x)),
    glance = map(models, broom::glance),
    results = map(models, broom::tidy)
  ) %>%
  select(-strap, -models) %>% 
  unnest(results) %>% 
  unnest(glance, names_repair = "universal") %>% 
  select(.id, r.squared, term, estimate)

bootstrap_results = 
  boot_straps %>% 
    mutate(
      term = if_else(term == "tmin", "tmin", "Intercept")) %>% 
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>% 
  mutate(
    log_b0_b1 = log(Intercept) + log(tmin)
  )
```

### Plotting

First let's look at r^2:

```{r}
bootstrap_results %>% 
  ggplot(aes(x = r.squared)) +
  geom_density() + 
  ggtitle("Distribution of R squared in 5,000 bootstrap samples")
```

Now looking at log(β^0∗β^1):

```{r}
bootstrap_results %>% 
  ggplot(aes(x = log_b0_b1)) +
  geom_density() + 
  ggtitle("Distribution of Log(B0 * B1) in 5,000 bootstrap samples")
```